{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wmok12/Thesis-research-minimumwage-employment/blob/main/Model_coding_construction_industry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwROSlKcaWDT"
      },
      "source": [
        "# Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime\n"
      ],
      "metadata": {
        "id": "n_wXdbjIHgtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PxejXy4YnYQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, ParameterGrid\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import shap\n",
        "\n",
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pCF2Ch5aiQV"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Thesis/Voorbereiding/cleaned_dataset_cps_construction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference used libraries"
      ],
      "metadata": {
        "id": "7efylTrzDU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print('numpy:',numpy.__version__)\n",
        "\n",
        "import pandas\n",
        "print('pandas:', pandas.__version__)\n",
        "\n",
        "import sklearn\n",
        "print('sklearn:', sklearn.__version__)\n",
        "\n",
        "import imblearn\n",
        "print('imblearn', imblearn.__version__)\n",
        "\n",
        "import xgboost\n",
        "print('xgboost', xgboost.__version__)\n",
        "\n",
        "import matplotlib\n",
        "print('matplotlib', matplotlib.__version__)\n",
        "\n",
        "import seaborn\n",
        "print('seaborn', seaborn.__version__)\n",
        "\n",
        "import scipy\n",
        "print('scipy', scipy.__version__)\n",
        "\n",
        "import lime\n",
        "import shap\n"
      ],
      "metadata": {
        "id": "jlTOpcNmDTn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show lime\n",
        "!pip show shap\n"
      ],
      "metadata": {
        "id": "W4Zem3ZBQPW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8HMm0AtawDi"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-ZKq7E2a2RC"
      },
      "outputs": [],
      "source": [
        "# Binary values\n",
        "dataset['LABFORCE_binary'] = dataset['LABFORCE'].map({'In labor force': 1, 'Not labor force': 0})\n",
        "dataset['SEX_binary'] = dataset['SEX'].map({'Male': 1, 'Female': 0})\n",
        "dataset['CLASSWKR_binary'] = dataset['CLASSWKR'].map({'Self-employed': 1, 'Work for wages': 0})\n",
        "dataset['EMPSTAT_binary'] = dataset['EMPSTAT'].map({'Unemployed': 1, 'Employed': 0})\n",
        "\n",
        "# Split the dataset in X and y\n",
        "X = dataset[['AGE', 'SEX_binary', 'MINIMUM WAGE', 'EDUC', 'CLASSWKR_binary', 'MARST', 'RACE', 'LABFORCE_binary']]\n",
        "y = dataset['EMPSTAT_binary']\n",
        "\n",
        "# Preprocessing\n",
        "numerical_features = ['AGE','MINIMUM WAGE']\n",
        "numerical_transformer = StandardScaler()\n",
        "\n",
        "binary_features = ['SEX_binary', 'CLASSWKR_binary', 'LABFORCE_binary']\n",
        "\n",
        "categorical_features_encode = ['MARST', 'RACE']\n",
        "categorical_transformer_encode = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "education_order = [['No schooling', 'Grades 1 t/m 4', 'Grades 5 or 6', 'Grades 7 or 8', 'Grade 9', 'Grade 10', 'Grade 11', 'Grade 12', 'High school diploma','Some college',  'AC degree, occupational program', 'AC degree, academic program', 'Bachelor\\'s degree', 'Master/s degree', 'Professional school degree', 'Doctorate degree']]\n",
        "categorical_features_label = ['EDUC']\n",
        "categorical_transformer_label = OrdinalEncoder(categories = education_order)\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat_encode', categorical_transformer_encode, categorical_features_encode),\n",
        "    ('cat_label', categorical_transformer_label, categorical_features_label),\n",
        "    ('binary', 'passthrough', binary_features),\n",
        "    ('num', numerical_transformer, numerical_features)\n",
        "])\n",
        "\n",
        "X = preprocessor.fit_transform(X)\n",
        "\n",
        "# Get the feature names after preprocessing\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Convert the transformed data back to DataFrames with proper feature names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split data into 80% training/validation data and 20% testing data\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Split data the training/validation data into 60% training data and 20% into validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state = 42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAoXceV9bG3p"
      },
      "source": [
        "# Logistic Regression (baseline) - addressing class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYQKbl2kHg2F"
      },
      "outputs": [],
      "source": [
        "# RFE selector based on EDA\n",
        "rfe_lr = RFE(estimator=LogisticRegression(max_iter=10000, random_state=42), n_features_to_select=2)\n",
        "\n",
        "# Logistic regression pipeline\n",
        "lr_model = ImbPipeline(steps=[\n",
        "    ('feature_selection', rfe_lr),\n",
        "    ('classifier', LogisticRegression(max_iter=10000, random_state=42))\n",
        "])\n",
        "\n",
        "# Setting parameters\n",
        "lr_param_grid = {\n",
        "    'classifier__solver': ['lbfgs', 'saga'],\n",
        "    'classifier__penalty': ['l2', 'elasticnet'],\n",
        "    'classifier__C': [0.1, 1],\n",
        "    'classifier__class_weight': [\n",
        "        {0: 1, 1: 2},\n",
        "        {0: 1, 1: 3}\n",
        "    ],\n",
        "    'classifier__l1_ratio': [0.5]\n",
        "}\n",
        "\n",
        "# Filtering invalid parameter combinations\n",
        "valid_params = []\n",
        "for params in ParameterGrid(lr_param_grid):\n",
        "    if params['classifier__penalty'] == 'elasticnet' and params['classifier__solver'] != 'saga':\n",
        "        continue\n",
        "    if params['classifier__penalty'] != 'elasticnet' and 'classifier__l1_ratio' in params:\n",
        "        continue\n",
        "    valid_params.append(params)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Scoring with weighted F1\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label=1),\n",
        "    'recall': make_scorer(recall_score, pos_label=1),\n",
        "    'f1_weighted': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "lr_grid_search = GridSearchCV(\n",
        "    estimator=lr_model,\n",
        "    param_grid=lr_param_grid,\n",
        "    cv=kf_cv,\n",
        "    scoring=scoring,\n",
        "    refit='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "# Train the model on the training data using GridSearchCV\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Results per fold\n",
        "cv_results = pd.DataFrame(lr_grid_search.cv_results_)\n",
        "\n",
        "# Columns for per fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1_weighted\" in col]\n",
        "\n",
        "# Best hyperparameters and scores per fold\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold{i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the  best hyperparameters and scores per fold\n",
        "print(\"Beste hyperparameters en scores per fold:\")\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Score: {result['best_score']:.4f}\")\n",
        "\n",
        "# Show the best final hyperparameters and score\n",
        "print(\"Best hyperparameters:\", lr_grid_search.best_params_)\n",
        "print(\"Best accuracy:\", lr_grid_search.best_score_)\n",
        "print(\"Best precision:\", lr_grid_search.cv_results_['mean_test_precision'][lr_grid_search.best_index_])\n",
        "print(\"Best recall:\", lr_grid_search.cv_results_['mean_test_recall'][lr_grid_search.best_index_])\n",
        "print(\"Best F1-score:\", lr_grid_search.cv_results_['mean_test_f1_weighted'][lr_grid_search.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on training data\n",
        "lr_y_pred_train = lr_grid_search.best_estimator_.predict(X_train)\n",
        "\n",
        "# Classification report for validation data\n",
        "print(\"Classification report for Training data - LR\")\n",
        "print(classification_report(y_train, lr_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "lr_y_pred_val = lr_grid_search.best_estimator_.predict(X_val)\n",
        "\n",
        "# Classification report for validation data\n",
        "print(\"Classification report for validation data - LR\")\n",
        "print(classification_report(y_val, lr_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "lr_y_pred_test = lr_grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Classification report for testing data\n",
        "print(\"Classification report for testing data - LR\")\n",
        "print(classification_report(y_test, lr_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_AcdT9-ji_S"
      },
      "source": [
        "# Random Forest - no hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xanKcM8tjhou"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Random Forest\n",
        "rf_model = ImbPipeline(steps=[\n",
        "    ('classifier', RandomForestClassifier(random_state = 42))\n",
        "])\n",
        "\n",
        "# Train random_search on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_model.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqt2yYufkDi7"
      },
      "source": [
        "# Random Forest - addressing imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMHWnfxBj-mj"
      },
      "outputs": [],
      "source": [
        "# Make an RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RFE to take the top 2 selected features\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=9)\n",
        "\n",
        "# Random Forest\n",
        "rf_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature_selection', rfe),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# Train random_search on the training data\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvWW-fJYKqMY"
      },
      "outputs": [],
      "source": [
        "# Haal de RFE-stap op uit de pipeline, let op: gebruik rf_pipeline in plaats van rf_model\n",
        "rfe_step = rf_pipeline.named_steps['feature_selection']\n",
        "\n",
        "# De 'support_' attribute bevat een booleaanse array die aangeeft welke features geselecteerd zijn\n",
        "selected_features_boolean = rfe_step.support_\n",
        "\n",
        "# Haal de geselecteerde feature-namen op (gebruik de juiste feature-namen na preprocessing)\n",
        "selected_feature_names = pd.Series(feature_names)[selected_features_boolean].tolist()\n",
        "\n",
        "# Toon de geselecteerde features\n",
        "print(\"Geselecteerde features door RFE:\", selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOsNHnmTjB1x"
      },
      "source": [
        "# Random Forest - hyperparameter tuning random and gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUkcL_nsUt1T"
      },
      "outputs": [],
      "source": [
        "# RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RFE to select the top 9 features\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=9)\n",
        "\n",
        "# Pipeline\n",
        "rf_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature_selection', rfe),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "# Scoring\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label = 1),\n",
        "    'recall': make_scorer(recall_score, pos_label = 1),\n",
        "    'f1_weighted': make_scorer(f1_score, average = \"weighted\")\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 5, 10],\n",
        "    'classifier__class_weight': ['balanced',\n",
        "     {0: 1, 1: 2},\n",
        "     {0: 1, 1: 3}]\n",
        "}\n",
        "\n",
        "\n",
        "# GridSearchCV\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator = rf_pipeline,\n",
        "    param_grid = rf_param_grid,\n",
        "    cv = kf_cv,\n",
        "    scoring = scoring,\n",
        "    refit = 'f1_weighted',\n",
        "    n_jobs = -1,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Results per fold\n",
        "cv_results = pd.DataFrame(rf_grid_search.cv_results_)\n",
        "\n",
        "# Columns for per-fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1\" in col]\n",
        "\n",
        "# For each fold finding the best hyperparameters\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold {i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the best hyperparameters per fold\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Weighted F1-Score: {result['best_score']:.4f}\")\n",
        "\n",
        "print(\"Best hyperparameters:\", rf_grid_search.best_params_)\n",
        "print(\"Best accuracy:\", rf_grid_search.best_score_)\n",
        "print(\"Best precision:\", rf_grid_search.cv_results_['mean_test_precision'][rf_grid_search.best_index_])\n",
        "print(\"Best recall:\", rf_grid_search.cv_results_['mean_test_recall'][rf_grid_search.best_index_])\n",
        "print(\"Best F1-score:\", rf_grid_search.cv_results_['mean_test_f1_weighted'][rf_grid_search.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_train = rf_grid_search.predict(X_train)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for training data - RF\")\n",
        "print(classification_report(y_train, rf_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_grid_search.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_grid_search.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest with hyperparameter tuning without class imbalance"
      ],
      "metadata": {
        "id": "zDuBqXqVAJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RFE to select the top 9 features\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=9)\n",
        "\n",
        "# Pipeline\n",
        "rf_pipeline = ImbPipeline(steps=[\n",
        "    ('feature_selection', rfe),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# General setting for RandomSearchCV en GridSearchCV\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "# Scoring\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label = 1),\n",
        "    'recall': make_scorer(recall_score, pos_label = 1),\n",
        "    'f1_weighted': make_scorer(f1_score, average = \"weighted\")\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# GridSearchCV\n",
        "rf_grid_search_2 = GridSearchCV(\n",
        "    estimator = rf_pipeline,\n",
        "    param_grid = rf_param_grid,\n",
        "    cv = kf_cv,\n",
        "    scoring = scoring,\n",
        "    refit = 'f1_weighted',\n",
        "    n_jobs = -1,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_grid_search_2.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Results per fold\n",
        "cv_results = pd.DataFrame(rf_grid_search_2.cv_results_)\n",
        "\n",
        "# Columns for per-fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1\" in col]\n",
        "\n",
        "# For each fold finding the beste hyperparameters\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold {i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the best hyperparameters per fold\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Weighted F1-Score: {result['best_score']:.4f}\")\n",
        "\n",
        "print(\"Best hyperparameters:\", rf_grid_search_2.best_params_)\n",
        "print(\"Best accuracy:\", rf_grid_search_2.best_score_)\n",
        "print(\"Best precision:\", rf_grid_search_2.cv_results_['mean_test_precision'][rf_grid_search_2.best_index_])\n",
        "print(\"Best recall:\", rf_grid_search_2.cv_results_['mean_test_recall'][rf_grid_search_2.best_index_])\n",
        "print(\"Best F1-score:\", rf_grid_search_2.cv_results_['mean_test_f1_weighted'][rf_grid_search_2.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_train = rf_grid_search_2.predict(X_train)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for training data - RF\")\n",
        "print(classification_report(y_train, rf_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_grid_search_2.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_grid_search_2.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ],
      "metadata": {
        "id": "m-vA22Jb_-QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FiA-v66Fls"
      },
      "source": [
        "# Feature importance - RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_lIo22P6C8G"
      },
      "outputs": [],
      "source": [
        "# changing the features to more readable names\n",
        "feature_name_mapping = {\n",
        "    'num__AGE': 'Age',\n",
        "    'num__MINIMUM WAGE': 'Minimum Wage',\n",
        "    'cat_label__EDUC': 'Education',\n",
        "    'binary__LABFORCE_binary': 'Labor Force Status',\n",
        "    'binary__CLASSWKR_binary': 'Class of Worker',\n",
        "    'binary__SEX_binary': 'Gender',\n",
        "    'cat_encode__MARST_Married': 'Marital Status - Married',\n",
        "    'cat_encode__RACE_White': 'Race - White',\n",
        "    'cat_encode__MARST_Never married': 'Marital Status - Never Married'\n",
        "}\n",
        "\n",
        "# Extract selected features from the fitted RFE\n",
        "fitted_rfe = rf_grid_search.best_estimator_.named_steps['feature_selection']\n",
        "fitted_rf = rf_grid_search.best_estimator_.named_steps['classifier']\n",
        "\n",
        "# Get the selected feature names using RFE\n",
        "selected_features = X_train.columns[fitted_rfe.support_]\n",
        "\n",
        "# Get feature importances from the fitted RandomForestClassifier\n",
        "feature_importances = fitted_rf.feature_importances_\n",
        "\n",
        "# Sort the features and their importance scores\n",
        "sorted_idx = np.argsort(feature_importances)\n",
        "sorted_features = selected_features[sorted_idx]\n",
        "sorted_importances = feature_importances[sorted_idx]\n",
        "\n",
        "# Rename the features using the mapping dictionary\n",
        "renamed_sorted_features = [feature_name_mapping.get(feat, feat) for feat in sorted_features]\n",
        "\n",
        "# Create a horizontal bar chart for feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(renamed_sorted_features, sorted_importances, color='#ADD8E6')\n",
        "plt.xlabel('Feature importance score', fontsize = 16)\n",
        "plt.ylabel('Features', fontsize = 16)\n",
        "plt.title('Feature importance - RF', fontsize = 20)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix displayed as heatmap"
      ],
      "metadata": {
        "id": "3EV4TZvO8Mg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix voor de testdata\n",
        "conf_matrix = confusion_matrix(y_test, rf_y_pred_test)\n",
        "\n",
        "# Plot de heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Employed', 'Unemployed'], yticklabels=['Employed', 'Unemployed'],annot_kws={\"size\": 16} )\n",
        "plt.xlabel('Predicted value', fontsize=18)\n",
        "plt.ylabel('True value', fontsize=18)\n",
        "plt.title('Confusion Matrix - RF', fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "doioXwpo3lX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs predicted - scatterplot"
      ],
      "metadata": {
        "id": "rGuc2Yfw38Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# actual vs predicted\n",
        "actual = y_test\n",
        "predicted = rf_y_pred_test\n",
        "\n",
        "classes = np.unique(actual)\n",
        "\n",
        "# Scatterplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for cls in classes:\n",
        "    plt.scatter(\n",
        "        np.where(actual == cls)[0],\n",
        "        predicted[actual == cls],\n",
        "        label=f\"Class {cls}\"\n",
        "    )\n",
        "\n",
        "plt.axhline(y=0.5, color=\"gray\", linestyle=\"--\", label=\"Decision boundary\")\n",
        "plt.xlabel(\"Sample index\", fontsize=18)\n",
        "plt.ylabel(\"Predicted value\", fontsize=18)\n",
        "plt.title(\"Actual vs Predicted - RF\", fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(loc=\"best\", fontsize = 12)\n"
      ],
      "metadata": {
        "id": "brrU_VGP4BBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME"
      ],
      "metadata": {
        "id": "foHzPGMT4m-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "i = 10  # Fixed index from the validation set, choose a specific index like 10\n",
        "\n",
        "# LIME explanation for RF without class imbalance\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train),\n",
        "    feature_names=X_train.columns,\n",
        "    class_names=['class_0', 'class_1'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "    data_row=X_val.iloc[i],\n",
        "    predict_fn=rf_grid_search_2.predict_proba,\n",
        "    labels=[0]\n",
        ")\n",
        "\n",
        "# Display the explanation with matplotlib\n",
        "fig = exp.as_pyplot_figure(label=0)\n",
        "\n",
        "# Adjust the title to make it more readable\n",
        "plt.title(\"LIME Explanation\", fontsize = 14)\n",
        "plt.ylabel(\"Features\", fontsize = 14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4qkHUEmIoEM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "i = 10\n",
        "\n",
        "# LIME explanation for RF with class imbalance\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train),\n",
        "    feature_names=X_train.columns,\n",
        "    class_names=['class_0', 'class_1'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "    data_row=X_val.iloc[i],\n",
        "    predict_fn=rf_grid_search.predict_proba,\n",
        "    labels=[0]\n",
        ")\n",
        "\n",
        "# Display the explanation with matplotlib\n",
        "fig = exp.as_pyplot_figure(label=0)\n",
        "\n",
        "# Adjust the title and feature names to make it more readable\n",
        "plt.title(\"LIME Explanation\", fontsize = 14)\n",
        "plt.ylabel(\"Features\", fontsize = 14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xARVuPCs_bFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP"
      ],
      "metadata": {
        "id": "FpfvvrqHDy2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stap 1: Verkrijg de geselecteerde features via RFE\n",
        "rfe_step = rf_grid_search_2.best_estimator_.named_steps['feature_selection']\n",
        "selected_features = X_train.columns[rfe_step.support_]\n",
        "\n",
        "# Stap 2: Neem een sample van 1000 rijen uit X_val en bijbehorende labels\n",
        "sample_size = 4000\n",
        "random_state = 42\n",
        "\n",
        "X_val_sample = X_val.sample(n=sample_size, random_state=random_state)\n",
        "y_val_sample = y_val.loc[X_val_sample.index]  # Zorg dat labels overeenkomen\n",
        "\n",
        "# Stap 3: Filter de sampled data voor de geselecteerde features\n",
        "X_val_selected = X_val_sample[selected_features]\n",
        "\n",
        "# Stap 4: Maak de SHAP Explainer voor de classifier\n",
        "explainer = shap.TreeExplainer(rf_grid_search_2.best_estimator_.named_steps['classifier'], feature_perturbation=\"interventional\")\n",
        "\n",
        "# Stap 5: Bereken SHAP waarden voor de sample\n",
        "shap_values = explainer.shap_values(X_val_selected)"
      ],
      "metadata": {
        "id": "b7-C4ZZXD-4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP Summary Plot als bar chart\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    features=X_val_selected,\n",
        "    feature_names=selected_features,\n",
        "    plot_type=\"bar\",\n",
        "    plot_size=(20, 50)  # Pas de grootte van de grafiek aan\n",
        ")\n"
      ],
      "metadata": {
        "id": "_ZtcxOq40WXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez5nXtyjvde-"
      },
      "source": [
        "# XGBoost - without hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnZPtV73vqRn"
      },
      "outputs": [],
      "source": [
        "# eXtreme Gradient Boosting (XGBoost)\n",
        "\n",
        "xgb_model = ImbPipeline(steps=[\n",
        "    ('classifier',XGBClassifier(random_state = 42))\n",
        "])\n",
        "\n",
        "# Train the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "xgb_y_pred_val = xgb_model.predict(X_val)\n",
        "\n",
        "# Create a DataFrame for actual vs predicted values\n",
        "a = pd.DataFrame({'Actual value': y_val, 'Predicted value': xgb_y_pred_val})\n",
        "print(a.head())\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - XGBoost\")\n",
        "print(classification_report(y_val, xgb_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "xgb_y_pred_test = xgb_model.predict(X_test)\n",
        "\n",
        "# Create a DataFrame for actual vs predicted values\n",
        "a = pd.DataFrame({'Actual value': y_test, 'Predicted value': xgb_y_pred_test})\n",
        "print(a.head())\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - XGBoost\")\n",
        "print(classification_report(y_test, xgb_y_pred_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30XFaVSmwH69"
      },
      "source": [
        "# XGBoost - adressing class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "korf0QbZwNk1"
      },
      "outputs": [],
      "source": [
        "# eXtreme Gradient Boosting (XGBoost)\n",
        "# Make the model\n",
        "xgb_model = XGBClassifier(random_state = 42)\n",
        "\n",
        "# Using RFE\n",
        "rfe_xgb = RFE(estimator = xgb_model, n_features_to_select = 11)\n",
        "\n",
        "xgb_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature selection', rfe_xgb),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# Train the model on the training data\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "xgb_y_pred_val = xgb_pipeline.predict(X_val)\n",
        "\n",
        "# Create a DataFrame for actual vs predicted values\n",
        "a = pd.DataFrame({'Actual value': y_val, 'Predicted value': xgb_y_pred_val})\n",
        "print(a.head())\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - XGBoost\")\n",
        "print(classification_report(y_val, xgb_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "xgb_y_pred_test = xgb_pipeline.predict(X_test)\n",
        "\n",
        "# Create a DataFrame for actual vs predicted values\n",
        "a = pd.DataFrame({'Actual value': y_test, 'Predicted value': xgb_y_pred_test})\n",
        "print(a.head())\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - XGBoost\")\n",
        "print(classification_report(y_test, xgb_y_pred_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5xLsy6AxXtu"
      },
      "source": [
        "# XGBoost - hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXbCdPG6P8kv"
      },
      "outputs": [],
      "source": [
        "# eXtreme Gradient Boosting (XGBoost)\n",
        "# Make the model\n",
        "xgb_model = XGBClassifier(random_state = 42)\n",
        "\n",
        "# Using RFE to select the op 11\n",
        "rfe_xgb = RFE(estimator = xgb_model, n_features_to_select = 11)\n",
        "\n",
        "xgb_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature selection', rfe_xgb),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "xgb_param_grid = {'classifier__n_estimators': [100, 200],\n",
        "              'classifier__max_depth': [3, 5, 7],\n",
        "              'classifier__scale_pos_weight': [1, 3]\n",
        "}\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "# Scoring\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label = 1),\n",
        "    'recall': make_scorer(recall_score, pos_label = 1),\n",
        "    'f1_weighted': make_scorer(f1_score, average = \"weighted\")\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "xgb_grid_search = GridSearchCV(\n",
        "    estimator = xgb_pipeline,\n",
        "    param_grid = xgb_param_grid,\n",
        "    cv = kf_cv,\n",
        "    scoring = scoring,\n",
        "    refit = 'f1_weighted',\n",
        "    n_jobs = -1,\n",
        "    verbose = 3\n",
        ")\n",
        "\n",
        "# Train the model on the training data\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Results per fold\n",
        "cv_results = pd.DataFrame(xgb_grid_search.cv_results_)\n",
        "\n",
        "# Columns for per-fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1\" in col]\n",
        "\n",
        "# For each fold finding the beste hyperparameters\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold {i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the best hyperparameters per fold\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Weighted F1-Score: {result['best_score']:.4f}\")\n",
        "\n",
        "print(\"Best hyperparameters:\", xgb_grid_search.best_params_)\n",
        "print(\"Best accuracy:\", xgb_grid_search.best_score_)\n",
        "print(\"Best precision:\", xgb_grid_search.cv_results_['mean_test_precision'][xgb_grid_search.best_index_])\n",
        "print(\"Best recall:\", xgb_grid_search.cv_results_['mean_test_recall'][xgb_grid_search.best_index_])\n",
        "print(\"Best F1-score:\", xgb_grid_search.cv_results_['mean_test_f1_weighted'][xgb_grid_search.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on validation data\n",
        "xgb_y_pred_train = xgb_grid_search.predict(X_train)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - XGBoost\")\n",
        "print(classification_report(y_train, xgb_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "xgb_y_pred_val = xgb_grid_search.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - XGBoost\")\n",
        "print(classification_report(y_val, xgb_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "xgb_y_pred_test = xgb_grid_search.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - XGBoost\")\n",
        "print(classification_report(y_test, xgb_y_pred_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVUZZgAa9L-I"
      },
      "source": [
        "# Feature importance - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBLBpXc3w2-8"
      },
      "outputs": [],
      "source": [
        "# Changing the features to more readable names\n",
        "feature_name_mapping = {\n",
        "    'num__AGE': 'Age',\n",
        "    'num__MINIMUM WAGE': 'Minimum Wage',\n",
        "    'cat_label__EDUC': 'Education',\n",
        "    'binary__LABFORCE_binary': 'Labor Force Status',\n",
        "    'binary__CLASSWKR_binary': 'Class of Worker',\n",
        "    'binary__SEX_binary': 'Gender',\n",
        "    'cat_encode__MARST_Married': 'Marital Status - Married',\n",
        "    'cat_encode__RACE_White': 'Race - White',\n",
        "    'cat_encode__MARST_Never married': 'Marital Status - Never Married',\n",
        "    'cat_encode__RACE_Others': 'Race - others',\n",
        "    'cat_encode__RACE_BAME': 'Race - BAME'\n",
        "}\n",
        "\n",
        "# Access the fitted RFE and XGBClassifier from the pipeline\n",
        "fitted_rfe = xgb_grid_search.best_estimator_.named_steps['feature selection']\n",
        "fitted_xgb = xgb_grid_search.best_estimator_.named_steps['classifier']\n",
        "\n",
        "# Get the selected features based on RFE\n",
        "selected_features = X_train.columns[fitted_rfe.support_]\n",
        "\n",
        "# Rename the features using the mapping dictionary\n",
        "renamed_features = [feature_name_mapping.get(feat, feat) for feat in selected_features]\n",
        "\n",
        "# Get feature importances from the XGBClassifier (for selected features)\n",
        "feature_importances = fitted_xgb.feature_importances_\n",
        "\n",
        "# Sort feature importances for better visualization\n",
        "sorted_idx = np.argsort(feature_importances)\n",
        "sorted_importances = feature_importances[sorted_idx]\n",
        "sorted_renamed_features = [renamed_features[idx] for idx in sorted_idx]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(sorted_renamed_features, sorted_importances, color='#ADD8E6')\n",
        "plt.xlabel('Feature importance score', fontsize=16)\n",
        "plt.ylabel('Features', fontsize =16)\n",
        "plt.title('Feature importance - XGBoost', fontsize = 20)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix displayed as heatmap"
      ],
      "metadata": {
        "id": "h_vlrDtE8ypK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for the test data\n",
        "conf_matrix = confusion_matrix(y_test, xgb_y_pred_test)\n",
        "\n",
        "# Plot de heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Employed', 'Unemployed'], yticklabels=['Employed', 'Unemployed'],annot_kws={\"size\": 16} )\n",
        "plt.xlabel('Predicted value', fontsize=18)\n",
        "plt.ylabel('True value', fontsize=18)\n",
        "plt.title('Confusion Matrix - XGBoost', fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "onYw4NpY8iVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs predicted - scatterplot"
      ],
      "metadata": {
        "id": "yQ-Y217-9HRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual vs predicted\n",
        "actual = y_test\n",
        "predicted = xgb_y_pred_test\n",
        "\n",
        "classes = np.unique(actual)\n",
        "\n",
        "# Scatterplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for cls in classes:\n",
        "    plt.scatter(\n",
        "        np.where(actual == cls)[0],\n",
        "        predicted[actual == cls],\n",
        "        label=f\"Class {cls}\"\n",
        "    )\n",
        "\n",
        "plt.axhline(y=0.5, color=\"gray\", linestyle=\"--\", label=\"Decision boundary\")\n",
        "plt.xlabel(\"Sample index\", fontsize=18)\n",
        "plt.ylabel(\"Predicted value\", fontsize=18)\n",
        "plt.title(\"Actual vs Predicted - XGBoost\", fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(loc=\"best\", fontsize = 12)\n"
      ],
      "metadata": {
        "id": "wL2Vflow8pHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R8HMm0AtawDi",
        "DAoXceV9bG3p",
        "n_AcdT9-ji_S",
        "xqt2yYufkDi7",
        "xOsNHnmTjB1x",
        "zDuBqXqVAJkC",
        "L8FiA-v66Fls",
        "3EV4TZvO8Mg0",
        "rGuc2Yfw38Ql",
        "foHzPGMT4m-0",
        "FpfvvrqHDy2f",
        "Ez5nXtyjvde-",
        "30XFaVSmwH69",
        "f5xLsy6AxXtu",
        "TVUZZgAa9L-I",
        "h_vlrDtE8ypK",
        "yQ-Y217-9HRF"
      ],
      "provenance": [],
      "mount_file_id": "1Radep79bJeDCvk23nWZ2NSbmtXHaUfj9",
      "authorship_tag": "ABX9TyNxAt6kgtSVZvsIYeQnxnCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}