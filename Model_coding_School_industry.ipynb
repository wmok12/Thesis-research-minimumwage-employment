{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wmok12/Thesis-research-minimumwage-employment/blob/main/Model_coding_School_industry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwROSlKcaWDT"
      },
      "source": [
        "# Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PxejXy4YnYQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, ParameterGrid\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pCF2Ch5aiQV"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Thesis/Voorbereiding/dataset_cps_Elementary and secondary schools.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8HMm0AtawDi"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-ZKq7E2a2RC"
      },
      "outputs": [],
      "source": [
        "# Binary values\n",
        "dataset['LABFORCE_binary'] = dataset['LABFORCE'].map({'In labor force': 1, 'Not labor force': 0})\n",
        "dataset['SEX_binary'] = dataset['SEX'].map({'Male': 1, 'Female': 0})\n",
        "dataset['CLASSWKR_binary'] = dataset['CLASSWKR'].map({'Self-employed': 1, 'Work for wages': 0})\n",
        "dataset['EMPSTAT_binary'] = dataset['EMPSTAT'].map({'Unemployed': 1, 'Employed': 0})\n",
        "\n",
        "# Split the dataset in X and y\n",
        "X = dataset[['AGE', 'SEX_binary', 'MINIMUM WAGE', 'EDUC', 'CLASSWKR_binary', 'MARST', 'RACE', 'LABFORCE_binary']]\n",
        "y = dataset['EMPSTAT_binary']\n",
        "\n",
        "# Preprocessing\n",
        "numerical_features = ['AGE','MINIMUM WAGE']\n",
        "numerical_transformer = StandardScaler()\n",
        "\n",
        "binary_features = ['SEX_binary', 'CLASSWKR_binary', 'LABFORCE_binary']\n",
        "\n",
        "categorical_features_encode = ['MARST', 'RACE']\n",
        "categorical_transformer_encode = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "education_order = [['No schooling', 'Grades 1 t/m 4', 'Grades 5 or 6', 'Grades 7 or 8', 'Grade 9', 'Grade 10', 'Grade 11', 'Grade 12', 'High school diploma','Some college',  'AC degree, occupational program', 'AC degree, academic program', 'Bachelor\\'s degree', 'Master/s degree', 'Professional school degree', 'Doctorate degree']]\n",
        "categorical_features_label = ['EDUC']\n",
        "categorical_transformer_label = OrdinalEncoder(categories = education_order)\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat_encode', categorical_transformer_encode, categorical_features_encode),\n",
        "    ('cat_label', categorical_transformer_label, categorical_features_label),\n",
        "    ('binary', 'passthrough', binary_features),\n",
        "    ('num', numerical_transformer, numerical_features)\n",
        "])\n",
        "\n",
        "X = preprocessor.fit_transform(X)\n",
        "\n",
        "# Get the feature names after preprocessing\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Convert the transformed data back to DataFrames with proper feature names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split data into 80% training/validation data and 20% testing data\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Split data the training/validation data into 60% training data and 20% into validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state = 42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAoXceV9bG3p"
      },
      "source": [
        "# Logistic Regression (baseline) - addressing class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYQKbl2kHg2F"
      },
      "outputs": [],
      "source": [
        "# RFE selector based on EDA\n",
        "rfe_lr = RFE(estimator=LogisticRegression(max_iter=10000, random_state=42), n_features_to_select=2)\n",
        "\n",
        "# Logistic regression pipeline\n",
        "lr_model = ImbPipeline(steps=[\n",
        "    ('feature_selection', rfe_lr),\n",
        "    ('classifier', LogisticRegression(max_iter=10000, random_state=42))\n",
        "])\n",
        "\n",
        "# Setting parameters\n",
        "lr_param_grid = {\n",
        "    'classifier__solver': ['lbfgs', 'saga'],\n",
        "    'classifier__penalty': ['l2', 'elasticnet'],\n",
        "    'classifier__C': [0.1, 1],\n",
        "    'classifier__class_weight': [\n",
        "        {0: 1, 1: 2},\n",
        "        {0: 1, 1: 3}\n",
        "    ],\n",
        "    'classifier__l1_ratio': [0.5]\n",
        "}\n",
        "\n",
        "# Filtering invalid parameter combinations\n",
        "valid_params = []\n",
        "for params in ParameterGrid(lr_param_grid):\n",
        "    if params['classifier__penalty'] == 'elasticnet' and params['classifier__solver'] != 'saga':\n",
        "        continue\n",
        "    if params['classifier__penalty'] != 'elasticnet' and 'classifier__l1_ratio' in params:\n",
        "        continue\n",
        "    valid_params.append(params)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Scoring with weighted F1-score\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label=1),\n",
        "    'recall': make_scorer(recall_score, pos_label=1),\n",
        "    'f1_weighted': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "lr_grid_search = GridSearchCV(\n",
        "    estimator=lr_model,\n",
        "    param_grid=lr_param_grid,\n",
        "    cv=kf_cv,\n",
        "    scoring=scoring,\n",
        "    refit='f1_weighted',  # Optimalisatie voor weighted F1-score\n",
        "    n_jobs=-1,\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "# Train the model on the training data using GridSearchCV\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Resultats per fold\n",
        "cv_results = pd.DataFrame(lr_grid_search.cv_results_)\n",
        "\n",
        "# Columns for per-fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1_weighted\" in col]\n",
        "\n",
        "# Best hyperparameters and scores per fold\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold{i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the best hyperparameters and scores per fold\n",
        "print(\"Beste hyperparameters en scores per fold:\")\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Score: {result['best_score']:.4f}\")\n",
        "\n",
        "# Show the best final hyperparameters and score\n",
        "print(\"Best hyperparameters:\", lr_grid_search.best_params_)\n",
        "print(\"Best accuracy:\", lr_grid_search.best_score_)\n",
        "print(\"Best precision:\", lr_grid_search.cv_results_['mean_test_precision'][lr_grid_search.best_index_])\n",
        "print(\"Best recall:\", lr_grid_search.cv_results_['mean_test_recall'][lr_grid_search.best_index_])\n",
        "print(\"Best F1-score:\", lr_grid_search.cv_results_['mean_test_f1_weighted'][lr_grid_search.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on training data\n",
        "lr_y_pred_train = lr_grid_search.best_estimator_.predict(X_train)\n",
        "\n",
        "# Classification report for validation data\n",
        "print(\"Classification report for Training data - LR\")\n",
        "print(classification_report(y_train, lr_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "lr_y_pred_val = lr_grid_search.best_estimator_.predict(X_val)\n",
        "\n",
        "# Classification report for validation data\n",
        "print(\"Classification report for validation data - LR\")\n",
        "print(classification_report(y_val, lr_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "lr_y_pred_test = lr_grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Classification report for testing data\n",
        "print(\"Classification report for testing data - LR\")\n",
        "print(classification_report(y_test, lr_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_AcdT9-ji_S"
      },
      "source": [
        "# Random Forest - no hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xanKcM8tjhou"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Random Forest\n",
        "rf_model = ImbPipeline(steps=[\n",
        "    ('classifier', RandomForestClassifier(random_state = 42))\n",
        "])\n",
        "\n",
        "# Train random_search on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_model.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqt2yYufkDi7"
      },
      "source": [
        "# Random Forest - addressing imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMHWnfxBj-mj"
      },
      "outputs": [],
      "source": [
        "# RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RFE to select the top 2 features\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=9)\n",
        "\n",
        "# Random Forest\n",
        "rf_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature_selection', rfe),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# Train random_search on the training data\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvWW-fJYKqMY"
      },
      "outputs": [],
      "source": [
        "# Take the RFE-stap out of the pipeline\n",
        "rfe_step = rf_pipeline.named_steps['feature_selection']\n",
        "\n",
        "selected_features_boolean = rfe_step.support_\n",
        "selected_feature_names = pd.Series(feature_names)[selected_features_boolean].tolist()\n",
        "\n",
        "# Display the selected features\n",
        "print(\"Geselecteerde features door RFE:\", selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOsNHnmTjB1x"
      },
      "source": [
        "# Random Forest - hyperparameter tuning random and gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUkcL_nsUt1T"
      },
      "outputs": [],
      "source": [
        "# RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RFE\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=9)\n",
        "\n",
        "# Pipeline\n",
        "rf_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state = 42)),\n",
        "    ('feature_selection', rfe),\n",
        "    ('classifier', rf_model)\n",
        "])\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf_cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "# Scoring\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, pos_label = 1),\n",
        "    'recall': make_scorer(recall_score, pos_label = 1),\n",
        "    'f1_weighted': make_scorer(f1_score, average = \"weighted\")\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 5, 10],\n",
        "    'classifier__class_weight': ['balanced',\n",
        "     {0: 1, 1: 2},\n",
        "     {0: 1, 1: 3}]\n",
        "}\n",
        "\n",
        "\n",
        "# GridSearchCV\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator = rf_pipeline,\n",
        "    param_grid = rf_param_grid,\n",
        "    cv = kf_cv,               # Using K-Fold Cross-Validation as cv\n",
        "    scoring = scoring,\n",
        "    refit = 'f1_weighted',    # f1 needs to be trained again\n",
        "    n_jobs = -1,              # -1 will use all available processors\n",
        "    verbose = 1               # tijd besparen\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Results per fold\n",
        "cv_results = pd.DataFrame(rf_grid_search.cv_results_)\n",
        "\n",
        "# Columns for per-fold scores\n",
        "fold_columns = [col for col in cv_results.columns if \"split\" in col and \"test_f1\" in col]\n",
        "\n",
        "# For each fold finding the best parameters\n",
        "best_params_per_fold = {}\n",
        "for i, fold_col in enumerate(fold_columns):\n",
        "    best_index = cv_results[fold_col].idxmax()\n",
        "    best_params_per_fold[f\"fold {i+1}\"] = {\n",
        "        \"best_params\": {param: cv_results.loc[best_index, param] for param in cv_results.columns if param.startswith(\"param_\")},\n",
        "        \"best_score\": cv_results.loc[best_index, fold_col]\n",
        "    }\n",
        "\n",
        "# Display the beste hyperparameters per fold\n",
        "for fold, result in best_params_per_fold.items():\n",
        "    print(f\"{fold}:\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  Best Weighted F1-Score: {result['best_score']:.4f}\")\n",
        "\n",
        "print(\"Best hyperparameters:\", rf_grid_search.best_params_)\n",
        "print(\"Best accuracy:\", rf_grid_search.best_score_)\n",
        "print(\"Best precision:\", rf_grid_search.cv_results_['mean_test_precision'][rf_grid_search.best_index_])\n",
        "print(\"Best recall:\", rf_grid_search.cv_results_['mean_test_recall'][rf_grid_search.best_index_])\n",
        "print(\"Best F1-score:\", rf_grid_search.cv_results_['mean_test_f1_weighted'][rf_grid_search.best_index_])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Training data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_train = rf_grid_search.predict(X_train)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for training data - RF\")\n",
        "print(classification_report(y_train, rf_y_pred_train))\n",
        "\n",
        "# Validation data\n",
        "# Make prediction on validation data\n",
        "rf_y_pred_val = rf_grid_search.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for validation data - RF\")\n",
        "print(classification_report(y_val, rf_y_pred_val))\n",
        "\n",
        "# Testing data\n",
        "# Make prediction on testing data\n",
        "rf_y_pred_test = rf_grid_search.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification report for testing data - RF\")\n",
        "print(classification_report(y_test, rf_y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FiA-v66Fls"
      },
      "source": [
        "# Feature importance - RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_lIo22P6C8G"
      },
      "outputs": [],
      "source": [
        "# changing the features to more readable names\n",
        "feature_name_mapping = {\n",
        "    'num__AGE': 'Age',\n",
        "    'num__MINIMUM WAGE': 'Minimum Wage',\n",
        "    'cat_label__EDUC': 'Education',\n",
        "    'binary__LABFORCE_binary': 'Labor Force Status',\n",
        "    'binary__CLASSWKR_binary': 'Class of Worker',\n",
        "    'binary__SEX_binary': 'Gender',\n",
        "    'cat_encode__MARST_Married': 'Marital Status - Married',\n",
        "    'cat_encode__RACE_White': 'Race - White',\n",
        "    'cat_encode__MARST_Never married': 'Marital Status - Never Married'\n",
        "}\n",
        "\n",
        "# Extract selected features from the fitted RFE\n",
        "fitted_rfe = rf_grid_search.best_estimator_.named_steps['feature_selection']\n",
        "fitted_rf = rf_grid_search.best_estimator_.named_steps['classifier']\n",
        "\n",
        "# Get the selected feature names using RFE\n",
        "selected_features = X_train.columns[fitted_rfe.support_]\n",
        "\n",
        "# Get feature importances from the fitted RandomForestClassifier\n",
        "feature_importances = fitted_rf.feature_importances_\n",
        "\n",
        "# Sort the features and their importance scores\n",
        "sorted_idx = np.argsort(feature_importances)\n",
        "sorted_features = selected_features[sorted_idx]\n",
        "sorted_importances = feature_importances[sorted_idx]\n",
        "\n",
        "# Rename the features using the mapping dictionary\n",
        "renamed_sorted_features = [feature_name_mapping.get(feat, feat) for feat in sorted_features]\n",
        "\n",
        "# Create a horizontal bar chart for feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(renamed_sorted_features, sorted_importances, color='#ADD8E6')\n",
        "plt.xlabel('Feature importance score', fontsize = 16)\n",
        "plt.ylabel('Features', fontsize = 16)\n",
        "plt.title('Feature importance - RF', fontsize = 20)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix displayed as heatmap"
      ],
      "metadata": {
        "id": "3EV4TZvO8Mg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for the test data\n",
        "conf_matrix = confusion_matrix(y_test, rf_y_pred_test)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Employed', 'Unemployed'], yticklabels=['Employed', 'Unemployed'],annot_kws={\"size\": 16} )\n",
        "plt.xlabel('Predicted value', fontsize=18)\n",
        "plt.ylabel('True value', fontsize=18)\n",
        "plt.title('Confusion Matrix - RF', fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "doioXwpo3lX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs predicted - scatterplot"
      ],
      "metadata": {
        "id": "rGuc2Yfw38Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual en predicted values\n",
        "actual = y_test\n",
        "predicted = rf_y_pred_test\n",
        "\n",
        "classes = np.unique(actual)\n",
        "\n",
        "# Scatterplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for cls in classes:\n",
        "    plt.scatter(\n",
        "        np.where(actual == cls)[0],\n",
        "        predicted[actual == cls],\n",
        "        label=f\"Class {cls}\"\n",
        "    )\n",
        "\n",
        "plt.axhline(y=0.5, color=\"gray\", linestyle=\"--\", label=\"Decision boundary\")\n",
        "plt.xlabel(\"Sample index\", fontsize=18)\n",
        "plt.ylabel(\"Predicted value\", fontsize=18)\n",
        "plt.title(\"Actual vs Predicted - RF\", fontsize=20)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.legend(loc=\"best\", fontsize = 12)\n"
      ],
      "metadata": {
        "id": "brrU_VGP4BBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qwROSlKcaWDT",
        "R8HMm0AtawDi",
        "DAoXceV9bG3p",
        "n_AcdT9-ji_S",
        "xqt2yYufkDi7",
        "xOsNHnmTjB1x",
        "L8FiA-v66Fls",
        "3EV4TZvO8Mg0",
        "rGuc2Yfw38Ql"
      ],
      "provenance": [],
      "mount_file_id": "1fd_YUFwjOtpBMFL9M81dggVbg4_9EzWi",
      "authorship_tag": "ABX9TyNCNo2gxs7NT/yZf5j9iz0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}